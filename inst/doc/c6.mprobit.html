<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>6. Multinomial probit</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">6. Multinomial probit</h1>



<div id="the-model" class="section level2">
<h2>The model</h2>
<p>The multinomial probit is obtained with the same modeling that we used while presenting the random utility model. The utility of an alternative is still the sum of two components : <span class="math inline">\(U_j = V_j + \epsilon_j\)</span>.</p>
<p>but the joint distribution of the error terms is now a multivariate normal with mean 0 and with a matrix of covariance denoted <span class="math inline">\(\Omega\)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>Alternative <span class="math inline">\(l\)</span> is chosen if : <span class="math display">\[
\left\{
\begin{array}{rcl}
U_1-U_l&amp;=&amp;(V_1-V_l)+(\epsilon_1-\epsilon_l)&lt;0\\
U_2-U_l&amp;=&amp;(V_2-V_l)+(\epsilon_2-\epsilon_l)&lt;0\\
 &amp; \vdots &amp;  \\
U_J-U_l&amp;=&amp;(V_J-V_l)+(\epsilon_J-\epsilon_l)&lt;0\\
\end{array}
\right.
\]</span></p>
<p>wich implies, denoting <span class="math inline">\(V^l_j=V_j-V_l\)</span> :</p>
<p><span class="math display">\[
\left\{
\begin{array}{rclrcl}
  \epsilon^l_1 &amp;=&amp; (\epsilon_1-\epsilon_l) &amp;&lt;&amp; - V^l_1\\
  \epsilon^l_2 &amp;=&amp; (\epsilon_2-\epsilon_l) &amp;&lt;&amp; - V^l_2\\
  &amp;\vdots &amp; &amp; \vdots &amp;  \\
  \epsilon^l_J &amp;=&amp; (\epsilon_J-\epsilon_l) &amp;&lt;&amp; - V^l_J\\
\end{array}
\right.
\]</span></p>
<p>The initial vector of errors <span class="math inline">\(\epsilon\)</span> are transformed using the following transformation :</p>
<p><span class="math display">\[\epsilon^l = M^l \epsilon\]</span></p>
<p>where the transformation matrix <span class="math inline">\(M^l\)</span> is a <span class="math inline">\((J-1) \times J\)</span> matrix obtained by inserting in an identity matrix a <span class="math inline">\(l^{\mbox{th}}\)</span> column of <span class="math inline">\(-1\)</span>. For example, if <span class="math inline">\(J = 4\)</span> and <span class="math inline">\(l = 3\)</span> :</p>
<p><span class="math display">\[
M^3 = 
\left(
\begin{array}{cccc}
1 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 1 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 1 \\
\end{array}
\right)
\]</span></p>
<p>The covariance matrix of the error differences is obtained using the following matrix :</p>
<p><span class="math display">\[
\mbox{V}\left(\epsilon^l\right)=\mbox{V}\left(M^l\epsilon\right)
=
M^l\mbox{V}\left(\epsilon\right){M^l}^{\top}
=
M^l\Omega{M^l}^{\top}
\]</span></p>
<p>The probability of choosing <span class="math inline">\(l\)</span> is then :</p>
<p><span class="math display">\[\begin{equation}
P_l =\mbox{P}(\epsilon^l_1&lt;-V_1^l \;\&amp;\; \epsilon^l_2&lt;-V_2^l \;\&amp;\; ... \; \epsilon^l_J&lt;-V_J^l)
\end{equation}\]</span></p>
<p>with the hypothesis of distribution, this writes :</p>
<p><span class="math display">\[\begin{equation}
P_l = \int_{-\infty}^{-V_1^l}\int_{-\infty}^{-V_2^l}...\int_{-\infty}^{-V_J^l}\phi(\epsilon^l)
d\epsilon^l_1 d\epsilon^l_2... d^l_J
\end{equation}\]</span></p>
<p>with :</p>
<p><span class="math display">\[\begin{equation}
\phi\left(\epsilon^l\right)=\frac{1}{(2\pi)^{(J-1)/2}\mid\Omega^l\mid^{1/2}}
e^{-\frac{1}{2}\epsilon^l{\Omega^l}^{-1}\epsilon^l}
\end{equation}\]</span></p>
<p>Two problems arise with this model :</p>
<ul>
<li>the first one is that the identified parameters are the elements of <span class="math inline">\(\Omega^l\)</span> and not of <span class="math inline">\(\Omega\)</span>. We must then carefully investigate the meanings of these elements.</li>
<li>the second one is that the probability is a <span class="math inline">\(J-1\)</span> integral, which should be numerically computed. The relevant strategy in this context is to use simulations.</li>
</ul>
</div>
<div id="identification" class="section level2">
<h2>Identification</h2>
<p>The meaning-full parameters are those of the covariance matrix of the error <span class="math inline">\(\Omega\)</span>. For example, with <span class="math inline">\(J = 3\)</span> :</p>
<p><span class="math display">\[
\Omega =
\left(
\begin{array}{ccc}
\sigma_{11} &amp; \sigma_{12} &amp; \sigma_{13}  \\
\sigma_{21} &amp; \sigma_{22} &amp; \sigma_{23} \\
\sigma_{31} &amp; \sigma_{32} &amp; \sigma_{33} \\
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\Omega^1 = M^1 \Omega {M^1}^{\top}=
\left(
\begin{array}{cc}
\sigma_{11}+\sigma_{22}-2\sigma_{12} &amp; \sigma_{11} + \sigma_{23} - \sigma_{12} -\sigma_{13} \\
\sigma_{11}+\sigma_{23}- \sigma_{12} - \sigma_{13} &amp; \sigma_{11} + \sigma_{33} - 2 \sigma_{13} \\
\end{array}
\right)
\]</span></p>
<p>The overall scale of utility being unidentified, one has to impose the value of one of the variance, for example the first one is fixed to 1. We then have :</p>
<p><span class="math display">\[ \Omega^1 = \left( \begin{array}{cc} 1 &amp; \frac{\sigma_{11}+
\sigma_{23} - \sigma_{12}
-\sigma_{13}}{\sigma_{11}+\sigma_{22}-2\sigma_{12}} \\
\frac{\sigma_{11}+\sigma_{23}- \sigma_{12} -
\sigma_{13}}{\sigma_{11}+\sigma_{22}-2\sigma_{12}} &amp;
\frac{\sigma_{11} + \sigma_{33} - 2
\sigma_{13}}{\sigma_{11}+\sigma_{22}-2\sigma_{12}} \\ \end{array}
\right) 
\]</span></p>
<p>Therefore, out the 6 structural parameters of the covariance matrix, only 3 can be identified. Moreover, it’s almost impossible to interpret these parameters.</p>
<p>More generally, with <span class="math inline">\(J\)</span> alternatives, the number of the parameters of the covariance matrix is <span class="math inline">\((J+1)\times J/2\)</span> and the number of identified parameters is <span class="math inline">\(J\times(J-1)/2-1\)</span>.</p>
</div>
<div id="simulations" class="section level2">
<h2>Simulations</h2>
<p>Let <span class="math inline">\(L^l\)</span> be the Choleski decomposition of the covariance matrix of the error differences :</p>
<p><span class="math display">\[
\Omega^l=L^l {L^l}^{\top}
\]</span></p>
<p>This matrix is a lower triangular matrix of dimension <span class="math inline">\((J-1)\)</span> :</p>
<p><span class="math display">\[
L^l=
\left(
\begin{array}{ccccc}
l_{11} &amp; 0 &amp; 0 &amp;... &amp; 0 \\
l_{21} &amp; l_{22} &amp; 0 &amp; ... &amp; 0 \\
l_{31} &amp; l_{32} &amp; l_{33} &amp; ... &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
l_{(J-1)1} &amp; l_{(J-1)2} &amp; l_{(J-1)3} &amp; ... &amp; l_{(J-1)(J-1)} \\
\end{array}
\right)
\]</span></p>
<p>Let <span class="math inline">\(\eta\)</span> be a vector of standard normal deviates :</p>
<p><span class="math display">\[
\eta \sim N(0, I)
\]</span></p>
<p>Therefore, we have :</p>
<p><span class="math display">\[
\mbox{V}\left(L^l\eta\right)=L^lV(\eta){L^l}^{\top}=L^lI{L^l}^{\top}=\Omega^l
\]</span></p>
<p>Therefore, if we draw a vector of standard normal deviates <span class="math inline">\(\eta\)</span> and apply to it this transformation, we get a realization of <span class="math inline">\(\epsilon^l\)</span>.</p>
<p>This joint probability can be written as a product of conditional and marginal probabilities :</p>
<p><span class="math display">\[
\begin{array}{rcl}
  P_l &amp;=&amp; \mbox{P}(\epsilon^l_1&lt;- V_1^l \;\&amp;\; \epsilon^l_2&lt;-V_2^l \;\&amp;\; ... \;\&amp;\; \epsilon^l_J&lt;-V_J^l))\\
  &amp;=&amp; \mbox{P}(\epsilon^l_1&lt;- V_1^l))\\
  &amp;\times&amp;\mbox{P}(\epsilon^l_2&lt;-V_2^l \mid \epsilon^l_1&lt;-V_1^l) \\
  &amp;\times&amp;\mbox{P}(\epsilon^l_3&lt;-V_3^l \mid \epsilon^l_1&lt;-V_1^l \;\&amp;\; \epsilon^l_2&lt;-V_2^l) \\
  &amp; \vdots &amp; \\
  &amp;\times&amp;\mbox{P}(\epsilon^l_J&lt;-V_J^l \mid \epsilon^l_1&lt;-V_1^l \;\&amp;\; ... \;\&amp;\; \epsilon^l_{J-1}&lt;-V_{J-1}^l)) \\
\end{array}
\]</span></p>
<p>The vector of error differences deviates is :</p>
<p><span class="math display">\[
\left(
\begin{array}{c}
  \epsilon^l_1 \\ \epsilon^l_2 \\ \epsilon^l_3 \\ \vdots \\ \epsilon^l_J
\end{array}
\right)
=
\left(
\begin{array}{ccccc}
l_{11} &amp; 0 &amp; 0 &amp;... &amp; 0 \\
l_{21} &amp; l_{22} &amp; 0 &amp; ... &amp; 0 \\
l_{31} &amp; l_{32} &amp; l_{33} &amp; ... &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
l_{(J-1)1} &amp; l_{(J-1)2} &amp; l_{(J-1)3} &amp; ... &amp; l_{(J-1)(J-1)} \\
\end{array}
\right)
\times
\left(
\begin{array}{c}
\eta_1 \\ \eta_2 \\ \eta_3 \\ \vdots \\ \eta_J
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\left(
\begin{array}{c}
  \epsilon^l_1 \\ \epsilon^l_2 \\ \epsilon^l_3 \\ \vdots \\ \epsilon^l_J
\end{array}
\right)
=
\left(
\begin{array}{l}
l_{11}\eta_1 \\ 
l_{21}\eta_1+l_{22}\eta_2 \\ 
l_{31}\eta_1+l_{32}\eta_2 + l_{33}\eta_3\\ 
\vdots \\ 
l_{(J-1)1}\eta_1+l_{(J-1)2}\eta_2+...+l_{(J-1)(J-1)}\eta_{J-1}
\end{array}
\right)
\]</span></p>
<p>Let’s now investigate the marginal and conditional probabilities :</p>
<ul>
<li>the first one is simply the marginal probability for a standard normal deviates, therefore we have : <span class="math inline">\(\mbox{P}(\epsilon^l_1&lt;-V_1^l) = \Phi\left(-\frac{V_1^l}{l_{11}}\right)\)</span></li>
<li>the second one is, for a given value of <span class="math inline">\(\eta_1\)</span> equal to <span class="math inline">\(\Phi\left(-\frac{V^l_2+l_{21}\eta_1}{l_{22}}\right)\)</span>. We then have to compute the mean of this expression for any value of <span class="math inline">\(\eta_1\)</span> lower than <span class="math inline">\(-\frac{V^l_1}{l_{11}}\)</span>. We then have, denoting <span class="math inline">\(\bar{\phi}_1\)</span> the truncated normal density : <span class="math display">\[\mbox{P}(\epsilon^l_2&lt;-V_2^l)=\int_{-\infty}^{-\frac{V^l_1}{l_{11}}}\Phi\left(-\frac{V^l_2+l_{21}\eta_1}{l_{22}}\right)
\bar{\phi}_1(\eta_1)d\eta_1\]</span></li>
<li>the third one is, for given values of <span class="math inline">\(\eta_1\)</span> and <span class="math inline">\(\eta_2\)</span> equal to : <span class="math inline">\(\Phi\left(-\frac{V^l_3+l_{31}\eta_1+l_{32}\eta_2}{l_{33}}\right)\)</span>. We then have : <span class="math display">\[\mbox{P}(\epsilon^l_3&lt;-V_3^l)=\int_{-\infty}^{-\frac{V^l_1}{l_{11}}}\int_{-\infty}^{-\frac{V^l_2+l_{21}\eta_1}{l_{22}}}
\Phi\left(-\frac{V^l_3+l_{31}\eta_1+l_{32}\eta_2}{l_{33}}\right)\bar{\phi}_1(\eta_1)\bar{\phi}_2(\eta_2)d\eta_1d\eta_2\]</span></li>
<li>and so on.</li>
</ul>
<p>This probabilities can easily be simulated by drawing numbers from a truncated normal distribution.</p>
<p>This so called GHK algorithm<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> (for Geweke, Hajivassiliou and Keane who developed this algorithm) can be described as follow :</p>
<p>Several points should be noted concerning this algorithm :</p>
<ul>
<li>the utility differences should be computed respective to the chosen alternative for each individual,</li>
<li>the Choleski decomposition used should relies on the same covariance matrix of the errors. One method to attained this goal is to start from a given difference, <em>e.g.</em> the difference respective with the first alternative. The vector of error difference is then <span class="math inline">\(\epsilon^1\)</span> and its covariance matrix is <span class="math inline">\(\Omega^1=L^1{L^1}^{\top}\)</span>. To apply a difference respective with an other alternative <span class="math inline">\(l\)</span>, we construct a matrix called <span class="math inline">\(S^l\)</span> which is obtained by using a <span class="math inline">\(J-2\)</span> identity matrix, adding a first row of 0 and inserting a column of <span class="math inline">\(-1\)</span> at the <span class="math inline">\(l-1^{\mbox{th}}\)</span> position. For example, with 4 alternatives and <span class="math inline">\(l=3\)</span>, we have : <span class="math display">\[S^3=
\left(
  \begin{array}{ccc}
    0 &amp; -1 &amp; 0 \\
    1 &amp; -1 &amp; 0 \\
    0 &amp; -1 &amp; 1 \\
  \end{array}
\right)
\]</span> The elements of the choleski decomposition of the covariance matrix is then obtained as follow : <span class="math display">\[
\Omega^l = S^l \Omega^1 {S^l}^{\top}=L^l {L^l}^{\top}
\]</span></li>
<li>to compute draws from a normal distribution truncated at <span class="math inline">\(a\)</span>, the following trick is used : take a draw <span class="math inline">\(\mu\)</span> from a uniform distribution (between 0 and 1) ; then <span class="math inline">\(\eta = \Phi^{-1}\left(\mu  \Phi(a)\right)\)</span> is a draw from a normal distribution truncated at <span class="math inline">\(a\)</span></li>
</ul>
</div>
<div id="applications" class="section level2">
<h2>Applications</h2>
<p>We use again the <code>Fishing</code> data frame, with only a subset of three alternatives used. The multinomial probit model is estimated using <code>mlogit</code> with the <code>probit</code> argument equal to <code>TRUE</code>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;mlogit&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;Fishing&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;mlogit&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>Fish &lt;-<span class="st"> </span><span class="kw">dfidx</span>(Fishing, <span class="dt">varying =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">9</span>, <span class="dt">choice =</span> <span class="st">&quot;mode&quot;</span>, <span class="dt">idnames =</span> <span class="kw">c</span>(<span class="st">&quot;chid&quot;</span>, <span class="st">&quot;alt&quot;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>Fish.mprobit &lt;-<span class="st"> </span><span class="kw">mlogit</span>(mode<span class="op">~</span>price <span class="op">|</span><span class="st"> </span>income <span class="op">|</span><span class="st"> </span>catch, Fish, <span class="dt">probit =</span> <span class="ot">TRUE</span>, <span class="dt">alt.subset=</span><span class="kw">c</span>(<span class="st">&#39;beach&#39;</span>, <span class="st">&#39;boat&#39;</span>,<span class="st">&#39;pier&#39;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">summary</span>(Fish.mprobit)</span></code></pre></div>
<pre><code>## 
## Call:
## mlogit(formula = mode ~ price | income | catch, data = Fish, 
##     alt.subset = c(&quot;beach&quot;, &quot;boat&quot;, &quot;pier&quot;), probit = TRUE)
## 
## Frequencies of alternatives:choice
##   beach    boat    pier 
## 0.18356 0.57260 0.24384 
## 
## bfgs method
## 14 iterations, 0h:0m:7s 
## g&#39;(-H)^-1g = 9.77E-07 
## gradient close to zero 
## 
## Coefficients :
##                     Estimate  Std. Error z-value  Pr(&gt;|z|)    
## (Intercept):boat  7.2514e-01  3.5809e-01  2.0250 0.0428661 *  
## (Intercept):pier  6.2393e-01  2.7396e-01  2.2774 0.0227617 *  
## price            -1.2154e-02  1.7697e-03 -6.8681 6.505e-12 ***
## income:boat       2.4005e-06  3.6698e-05  0.0654 0.9478448    
## income:pier      -6.5419e-05  4.0832e-05 -1.6022 0.1091198    
## catch:beach       1.5479e+00  4.3002e-01  3.5995 0.0003188 ***
## catch:boat        4.0010e-01  4.1600e-01  0.9618 0.3361595    
## catch:pier        1.2747e+00  5.5863e-01  2.2819 0.0224968 *  
## boat.pier         5.4570e-01  4.6263e-01  1.1795 0.2381809    
## pier.pier         6.9544e-01  2.9294e-01  2.3740 0.0175973 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Log-Likelihood: -478.43
## McFadden R^2:  0.32751 
## Likelihood ratio test : chisq = 465.99 (p.value = &lt; 2.22e-16)</code></pre>
</div>
<div id="bibliography" class="section level2 unnumbered">
<h2 class="unnumbered">Bibliography</h2>
<div id="refs" class="references hanging-indent">
<div id="ref-DAGA:79">
<p>Daganzo, C. 1979. <em>Multinomial Probit: The Theory and Its Application to Demand Forecasting</em>. Academic Press, New York.</p>
</div>
<div id="ref-GEWE:KEAN:RUNK:94">
<p>Geweke, J., M. Keane, and D. Runkle. 1994. “Alternative Computational Approaches to Inference in the Multinomial Probit Model.” <em>Review of Economics and Statistics</em> 76: 609–32.</p>
</div>
<div id="ref-HAUS:WISE:78">
<p>Hausman, J., and D. Wise. 1978. “A Conditional Probit Model for Qualitative Choice: Discrete Decisions Recognizing Interdemendence and Heterogeneous Preferences.” <em>Econometrica</em> 48: 403–29.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>see <span class="citation">(Hausman and Wise 1978)</span> and <span class="citation">(Daganzo 1979)</span><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>see for example <span class="citation">(Geweke, Keane, and Runkle 1994)</span>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
